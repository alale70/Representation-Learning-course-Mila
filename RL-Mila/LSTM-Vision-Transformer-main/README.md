# Text and Image Classification Using LSTM and Vision Transformer

### Representational Learning Course Project  
**Professor:** Aaron Courville  
**Institution:** Université de Montréal (UdeM)  

---

## Project Overview

This project was completed as part of the Deep Learning course at Polytechnique Montréal. The focus was on implementing and optimizing machine learning models for both text and image classification tasks. The project involved using Long Short-Term Memory (LSTM) networks for sequential language modeling and Vision Transformers (ViTs) for image classification.

---

## Objectives

1. **Text Classification**: Implement a Sequential Language Model using LSTM on the Wikitext dataset to predict text sequences.
2. **Image Classification**: Develop an image classifier using Vision Transformers on the CIFAR-10 dataset to categorize images into different classes.
3. **Hyperparameter Tuning**: Conduct extensive hyperparameter tuning to improve model performance and efficiency.

---

## Datasets

- **Wikitext Dataset**: A dataset consisting of text sequences used for training the LSTM model to predict the next word in a sequence.
- **CIFAR-10 Dataset**: A dataset of 60,000 32x32 color images in 10 classes, with 6,000 images per class, used for training the Vision Transformer model for image classification.
